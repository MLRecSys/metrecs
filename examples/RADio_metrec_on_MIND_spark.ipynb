{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fda62179-b1d3-498d-bd30-94ef80de57f8","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","23/07/20 23:47:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n","  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"]}],"source":["import os\n","import datetime\n","import time\n","import numpy as np\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from pyspark.sql import Row\n","from pyspark.sql.window import Window\n","from pyspark.sql import DataFrame as SparkDataFrame\n","import math\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from typing import Tuple, List\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('DEV-MIND').getOrCreate()\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["os.getcwd()\n","import sys\n","sys.path.insert(1, 'src/')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import metrecs"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from metrecs.utils import (\n","    harmonic_number,\n","    normalized_scaled_harmonic_number_series,\n","    compute_normalized_distribution_multiple_categories,\n","    opt_merge_max_mappings,\n","    avoid_distribution_misspecification,\n","    user_level_RADio_multicategorical\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"daa9e53b-becc-41a6-8110-5b19a7e79a26","showTitle":false,"title":""}},"outputs":[],"source":["def read_articles(path: str) -> SparkDataFrame:\n","    myschema = StructType([\n","                StructField('_c0', StringType(), True),\n","                StructField('newsid', StringType(), True),\n","                StructField('title', StringType(), True),\n","                StructField('category', StringType(), True),\n","                StructField('subcategory', StringType(), True),\n","                StructField('cat_subcat', ArrayType(StringType()), True),\n","                StructField('abstract', StringType(), True),\n","                StructField('publication_date', StringType(), True),\n","                StructField('url', StringType(), True)\n","            ])\n","    articles_df = spark.read.json(path, schema=myschema)\n","    articles_df = articles_df.withColumn('cat_as_list', F.udf(lambda x: [x], ArrayType(StringType()))(F.col('category')))\n","    print('Nr of unique articles', articles_df.select('newsid').distinct().count())\n","    return articles_df\n","\n","def read_behavior(path: str) -> SparkDataFrame:\n","    myschema = StructType([\n","        StructField('index', IntegerType(), True),\n","        StructField('user', StringType(), True),\n","        StructField('datetime', StringType(), True),\n","        StructField('behavior_string', StringType(), True),\n","        StructField('preselection_string', StringType(), True)\n","    ])\n","\n","    behaviors_presel_df = spark.read.csv(path, sep=\"\\t\", schema=myschema)#.drop('index')\n","    behaviors_presel_df = behaviors_presel_df.withColumn('behavior_array', F.split(F.col('behavior_string'),' ')).drop('behavior_string')\n","    behaviors_presel_df = behaviors_presel_df.withColumn('preselection_array', F.split(F.col('preselection_string'),' ')).drop('preselection_string')\n","    behaviors_presel_df = behaviors_presel_df.withColumn('pool_array', F.udf(lambda x: [i.split('-')[0] for i in x], ArrayType(StringType()))(F.col('preselection_array')))\n","\n","    return behaviors_presel_df\n","\n","def get_top_rec_ids_array(pred_df: SparkDataFrame, behaviors_presel_df: SparkDataFrame, top_k: int) -> SparkDataFrame:\n","    # Create a df with one row per user that contains only the top_k recommendations, that is the newsid and not the position of the preselection\n","    pred_df = pred_df.filter(F.col('size_list') >= top_k)\n","    pred_preselection_df = pred_df.join(behaviors_presel_df.select('index', 'user', 'preselection_array'), 'index')\n","    pred_preselection_df = pred_preselection_df.withColumn('pred_slice_id', F.udf(lambda x,y: [str(y[indx]).split('-')[0] for indx, ele in enumerate(x) if ele <= top_k],\n","                                                                                  ArrayType(StringType()))(F.col('pred_rank'), F.col('preselection_array')))\n","    pred_preselection_df = pred_preselection_df.drop('preselection_array').drop('pred_rank')\n","    return pred_preselection_df\n","\n","def read_predictions(path_predictions: str, behaviors_presel_df: SparkDataFrame, top_k: int, algo: str) -> Tuple[SparkDataFrame, SparkDataFrame]:\n","    pred_df = spark.read.json(path_predictions)\n","    pred_df = pred_df.withColumn('size_list', F.size('pred_rank'))\n","    pred_df = pred_df.withColumnRenamed('impr_index','index')\n","    pred_df = pred_df.withColumn('algo', F.lit(algo))\n","\n","    pred_preselection_df = get_top_rec_ids_array(pred_df, behaviors_presel_df, top_k = 2*top_k)\n","    return pred_df, pred_preselection_df\n"]},{"cell_type":"code","execution_count":6,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d0fceea2-28d9-4f95-a410-da0fa050f29f","showTitle":false,"title":""}},"outputs":[],"source":["def get_cat(df: SparkDataFrame, articles_df: SparkDataFrame, column: str, cat_column: str, top_at: int, slice_col: bool=False) -> SparkDataFrame:\n","    df_exploded = df.select('*', F.posexplode(column).alias('rank', 'newsid'))\\\n","                    .withColumn('rank', F.col('rank') + 1).drop(column)\n","\n","    df_cat = df_exploded.join(articles_df.select('newsid',cat_column).distinct(), 'newsid', how='inner')\n","    w = Window.partitionBy('index').orderBy('rank')\n","    df_cat = df_cat.withColumn('sorted_cat_list', F.collect_list(cat_column).over(w))\\\n","                   .withColumn('sorted_newsid_list', F.collect_list('newsid').over(w))\\\n","                   .groupby('index').agg(F.max('sorted_cat_list').alias('sorted_cat_list'),\n","                                                              F.max('sorted_newsid_list').alias('sorted_newsid_list'))\n","    df_cat = df_cat.join(df.drop(column), 'index')\n","    if slice_col:\n","        df_cat = df_cat.withColumn('sorted_cat_list', F.slice('sorted_cat_list', 1, top_at))\n","        df_cat = df_cat.withColumn('sorted_newsid_list', F.slice('sorted_newsid_list', 1, top_at))\n","        df_cat = df_cat.withColumn('size_cat_list', F.size('sorted_cat_list'))\n","        df_cat = df_cat.filter(F.col('size_cat_list') == top_at)\n","    return df_cat"]},{"cell_type":"code","execution_count":7,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2968b198-2313-49e7-8a82-1f4b4cc4a898","showTitle":false,"title":""}},"outputs":[],"source":["def get_classic_calibration(pred_preselection_df: SparkDataFrame, articles_df: SparkDataFrame, behaviors_presel_df: SparkDataFrame, cat_column: str, top_at: int) -> SparkDataFrame:\n","    df_cat = get_cat(pred_preselection_df, articles_df, 'pred_slice_id', cat_column, top_at, slice_col=True)\n","\n","    df_cat_history = get_cat(behaviors_presel_df.select('index', 'user', 'behavior_array'), articles_df, 'behavior_array', cat_column, top_at, slice_col=False)\n","    df_cat_history = df_cat_history.selectExpr('index', 'user', 'sorted_cat_list as history_cat_list', 'sorted_newsid_list as history_newsid_list')\n","\n","    df_rec_hist = df_cat.join(df_cat_history, ['index', 'user'], 'inner')\n","\n","    df_calibration = df_rec_hist.withColumn('calibration', F.udf(lambda x,y: user_level_RADio_multicategorical(x,y,list(normalized_scaled_harmonic_number_series(len(x)))), DoubleType())\\\n","                                                                 (F.col('sorted_cat_list'), F.col('history_cat_list')))\n","    return df_calibration\n","\n","def get_classic_representation(pred_preselection_df: SparkDataFrame, articles_df: SparkDataFrame, behaviors_presel_df: SparkDataFrame, cat_column: str, top_at: int) -> SparkDataFrame:\n","    df_cat = get_cat(pred_preselection_df, articles_df, 'pred_slice_id', cat_column, top_at, slice_col=True)\n","\n","    df_cat_pool = get_cat(behaviors_presel_df.select('index', 'user', 'pool_array'), articles_df, 'pool_array', cat_column, top_at, slice_col=False)\n","    df_cat_pool = df_cat_pool.selectExpr('index', 'user', 'sorted_cat_list as pool_cat_list', 'sorted_newsid_list as pool_newsid_list')\n","\n","    df_rec_pool = df_cat.join(df_cat_pool, ['index', 'user'], 'inner')\n","\n","    df_representation = df_rec_pool.withColumn('representation', F.udf(lambda x,y: user_level_RADio_multicategorical(x,y,list(normalized_scaled_harmonic_number_series(len(x)))), DoubleType())\\\n","                                                                       (F.col('sorted_cat_list'), F.col('pool_cat_list')))\n","    return df_representation\n","\n","def get_classic_fragmentation(pred_preselection_df: SparkDataFrame, articles_df: SparkDataFrame, cat_column: str, top_at: int, sample_size: float=0.05) -> SparkDataFrame:\n","    df_cat = get_cat(pred_preselection_df, articles_df, 'pred_slice_id', cat_column, top_at, slice_col=True)\n","    df_cat_cat = df_cat.crossJoin(df_cat.sample(sample_size)\\\n","                                  .selectExpr('index as other_index', 'user as other_user', 'sorted_cat_list as rec_cat_list', 'sorted_newsid_list as rec_newsid'))\n","    df_cat_cat = df_cat_cat.filter(F.col('index') != F.col('other_index'))\n","\n","    df_cat_cat = df_cat_cat.withColumn('fragmentation_detail', F.udf(lambda x,y: user_level_RADio_multicategorical(x,y,\n","                                                                                                                   list(normalized_scaled_harmonic_number_series(len(x))),\n","                                                                                                                   list(normalized_scaled_harmonic_number_series(len(y)))\n","                                                                                                                   ), DoubleType())\\\n","                                                                     (F.col('sorted_cat_list'), F.col('rec_cat_list')))\n","    df_fragmentation = df_cat_cat.groupby('index', 'user', 'algo').agg(F.mean('fragmentation_detail').alias('fragmentation'))\n","    return df_fragmentation"]},{"cell_type":"code","execution_count":8,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8d30e990-81b4-4626-a348-46881ded6c4a","showTitle":false,"title":""}},"outputs":[],"source":["\n","def read_recommendations_and_calculate_metrics(articles_df: SparkDataFrame, behaviors_presel_df: SparkDataFrame, sufix: str, algos: List[str], top_at: int=10, NR_BINS: int=200, path_prediction: str = 'examples/MIND/recommendations/') -> Tuple[SparkDataFrame, SparkDataFrame, SparkDataFrame, SparkDataFrame, SparkDataFrame, SparkDataFrame]:\n","    df_calibration_all = None\n","    df_representation_all = None\n","    df_fragmentation_all = None\n","\n","    for algo in algos:\n","        PATH_PRED = path_prediction+algo+'_pred_'+sufix+'.json'\n","        pred_df, pred_preselection_df = read_predictions(PATH_PRED, behaviors_presel_df, top_k = top_at, algo = algo)\n","\n","        df_calibration = get_classic_calibration(pred_preselection_df, articles_df, behaviors_presel_df, 'cat_as_list', top_at)\\\n","                            .select('index','user','calibration','algo')\n","        if df_calibration_all is None:\n","            df_calibration_all = df_calibration\n","        else:\n","            df_calibration_all = df_calibration_all.unionByName(df_calibration)\n","\n","        df_representation = get_classic_representation(pred_preselection_df, articles_df, behaviors_presel_df, 'cat_subcat', top_at)\\\n","                              .select('index','user','representation','algo')\n","        if df_representation_all is None:\n","            df_representation_all = df_representation\n","        else:\n","            df_representation_all = df_representation_all.unionByName(df_representation)\n","\n","        df_fragmentation = get_classic_fragmentation(pred_preselection_df, articles_df, 'cat_subcat', top_at, sample_size=0.02)\\\n","                              .select('index','user','fragmentation','algo')\n","        if df_fragmentation_all is None:\n","            df_fragmentation_all = df_fragmentation\n","        else:\n","            df_fragmentation_all = df_fragmentation_all.unionByName(df_fragmentation)\n","\n","    df_calibration_all = df_calibration_all.withColumn('calibration_bin', F.udf(lambda x: round(x*NR_BINS,0)/NR_BINS, DoubleType())(F.col('calibration')))\n","    calibration_agg_df = df_calibration_all.groupby('algo', 'calibration_bin').count()\n","\n","    df_representation_all = df_representation_all.withColumn('representation_bin', F.udf(lambda x: round(x*NR_BINS,0)/NR_BINS, DoubleType())(F.col('representation')))\n","    representation_agg_df = df_representation_all.groupby('algo', 'representation_bin').count()\n","\n","    fragmentation_agg_df = None\n","    if df_fragmentation_all is not None:\n","        df_fragmentation_all = df_fragmentation_all.withColumn('fragmentation_bin', F.udf(lambda x: round(x*NR_BINS,0)/NR_BINS, DoubleType())(F.col('fragmentation')))\n","        fragmentation_agg_df = df_fragmentation_all.groupby('algo', 'fragmentation_bin').count()\n","    return df_calibration_all, calibration_agg_df, df_representation_all, representation_agg_df, df_fragmentation_all, fragmentation_agg_df\n"]},{"cell_type":"code","execution_count":9,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"af51ef9c-63cb-4cf6-bf4d-87227da44aca","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 0:==============>                                            (2 + 6) / 8]\r"]},{"name":"stdout","output_type":"stream","text":["Nr of unique articles 68392\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["sufix = 'small'\n","PATH_BEHAVIOURS = 'examples/MIND/behaviors_'+sufix+'.tsv'\n","PATH_ARTICLES = 'examples/MIND/articles_large_narrow.json'\n","\n","articles_df = read_articles(path=PATH_ARTICLES)\n","behaviors_presel_df = read_behavior(path=PATH_BEHAVIOURS)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["PATH_PRED = 'examples/MIND/recommendations/'\n","df_calibration_all, calibration_agg_df, df_representation_all, representation_agg_df, df_fragmentation_all, fragmentation_agg_df = read_recommendations_and_calculate_metrics( articles_df, behaviors_presel_df, sufix = sufix, algos = ['lstur', 'pop', 'naml', 'npa', 'nrms'], top_at = 10, NR_BINS = 200, path_prediction=PATH_PRED)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/07/20 23:48:17 ERROR Executor: Exception in task 2.0 in stage 81.0 (TID 239) \n","org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n","    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n","    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n","    f, return_type = read_command(pickleSer, infile)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n","    command = serializer._read_with_length(file)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n","    return self.loads(obj)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 453, in loads\n","    return pickle.loads(obj, encoding=encoding)\n","ModuleNotFoundError: No module named 'metrecs'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n","\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n","\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage106.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","23/07/20 23:48:18 WARN TaskSetManager: Lost task 2.0 in stage 81.0 (TID 239) (192.168.1.6 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n","    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n","    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n","    f, return_type = read_command(pickleSer, infile)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n","    command = serializer._read_with_length(file)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n","    return self.loads(obj)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 453, in loads\n","    return pickle.loads(obj, encoding=encoding)\n","ModuleNotFoundError: No module named 'metrecs'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n","\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n","\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage106.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","23/07/20 23:48:18 ERROR TaskSetManager: Task 2 in stage 81.0 failed 1 times; aborting job\n","23/07/20 23:48:18 WARN TaskSetManager: Lost task 5.0 in stage 81.0 (TID 242) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:48:18 WARN TaskSetManager: Lost task 7.0 in stage 81.0 (TID 244) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:48:18 WARN TaskSetManager: Lost task 4.0 in stage 81.0 (TID 241) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:48:18 WARN TaskSetManager: Lost task 1.0 in stage 81.0 (TID 238) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:48:18 WARN TaskSetManager: Lost task 3.0 in stage 81.0 (TID 240) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:48:18 WARN TaskSetManager: Lost task 6.0 in stage 81.0 (TID 243) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:48:18 WARN TaskSetManager: Lost task 0.0 in stage 81.0 (TID 237) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:48:18 WARN TaskSetManager: Lost task 8.0 in stage 81.0 (TID 245) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n"]},{"ename":"PythonException","evalue":"\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 453, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'metrecs'\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/var/folders/pr/2nrw06tn1vb56m13zv7wlrsm0000gn/T/ipykernel_60449/1647239463.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_calibration_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1323\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 453, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'metrecs'\n"]}],"source":["df = df_calibration_all.toPandas()"]},{"cell_type":"code","execution_count":22,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3f874013-51af-4c73-b811-c81928d8c3af","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["23/07/20 23:46:38 ERROR Executor: Exception in task 2.0 in stage 96.0 (TID 248)\n","org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n","    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n","    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n","    f, return_type = read_command(pickleSer, infile)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n","    command = serializer._read_with_length(file)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n","    return self.loads(obj)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 453, in loads\n","    return pickle.loads(obj, encoding=encoding)\n","ModuleNotFoundError: No module named 'metrecs'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n","\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n","\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage106.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","23/07/20 23:46:38 WARN TaskSetManager: Lost task 2.0 in stage 96.0 (TID 248) (192.168.1.6 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n","    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n","    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n","    f, return_type = read_command(pickleSer, infile)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n","    command = serializer._read_with_length(file)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n","    return self.loads(obj)\n","  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 453, in loads\n","    return pickle.loads(obj, encoding=encoding)\n","ModuleNotFoundError: No module named 'metrecs'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n","\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n","\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage106.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","23/07/20 23:46:38 ERROR TaskSetManager: Task 2 in stage 96.0 failed 1 times; aborting job\n","23/07/20 23:46:38 WARN TaskSetManager: Lost task 3.0 in stage 96.0 (TID 249) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:46:38 WARN TaskSetManager: Lost task 5.0 in stage 96.0 (TID 251) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:46:38 WARN TaskSetManager: Lost task 1.0 in stage 96.0 (TID 247) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:46:38 WARN TaskSetManager: Lost task 0.0 in stage 96.0 (TID 246) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:46:38 WARN TaskSetManager: Lost task 6.0 in stage 96.0 (TID 252) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:46:38 WARN TaskSetManager: Lost task 4.0 in stage 96.0 (TID 250) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:46:38 WARN TaskSetManager: Lost task 7.0 in stage 96.0 (TID 253) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n","23/07/20 23:46:38 WARN TaskSetManager: Lost task 8.0 in stage 96.0 (TID 254) (192.168.1.6 executor driver): TaskKilled (Stage cancelled)\n"]},{"ename":"PythonException","evalue":"\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 453, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'metrecs'\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/var/folders/pr/2nrw06tn1vb56m13zv7wlrsm0000gn/T/ipykernel_55241/1941991277.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_calibration_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot the histogram thanks to the histplot function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"algo\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"lstur\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"calibration\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"calibration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lstur\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"algo\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nrms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"calibration\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"calibration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"green\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nrms\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1323\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 814, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/Users/joao.guedes/.pyenv/versions/3.7.10/envs/gflow/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 453, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'metrecs'\n"]}],"source":["\n","\n","# Plot the histogram thanks to the histplot function\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"lstur\"), [\"calibration\"]], bins=200, x=\"calibration\", color=\"red\", label=\"lstur\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"nrms\"), [\"calibration\"]], bins=200, x=\"calibration\", color=\"green\", label=\"nrms\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"naml\"), [\"calibration\"]], bins=200, x=\"calibration\", color=\"orange\", label=\"naml\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"npa\"), [\"calibration\"]], bins=200, x=\"calibration\", color=\"purple\", label=\"npa\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"pop\"), [\"calibration\"]], bins=200, x=\"calibration\", color=\"skyblue\", label=\"pop\", kde=False)\n","\n","plt.title(\"Calibration distribution\")\n","\n","plt.legend(['lstur', 'nrms', 'naml', 'npa', 'pop'])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":9,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f5f36c9f-99b4-48d1-9e2c-69bebd563b3e","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 152:=====>                                                  (1 + 9) / 10]\r"]},{"name":"stdout","output_type":"stream","text":["23/02/06 14:12:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 152:=================================>                      (6 + 4) / 10]\r"]},{"name":"stdout","output_type":"stream","text":["+-----+------------------+-------------------+\n","| algo|  mean_calibration| stddev_calibration|\n","+-----+------------------+-------------------+\n","|lstur|0.5846468118318968|0.14638931072017247|\n","|  pop|0.5838522364411468|0.14715790173498292|\n","| naml|0.5846088641726783|0.14783372378835283|\n","|  npa|0.5852890087411117|0.14621181689412424|\n","| nrms|0.5803803476447238|0.14594756226050673|\n","+-----+------------------+-------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df_calibration_all.groupby('algo').agg(F.mean('calibration').alias('mean_calibration'), \n","                                       F.stddev('calibration').alias('stddev_calibration')).show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df_representation_all.toPandas()\n","\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"lstur\"), [\"representation\"]], bins=200, x=\"representation\", color=\"red\", label=\"lstur\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"nrms\"), [\"representation\"]], bins=200, x=\"representation\", color=\"green\", label=\"nrms\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"naml\"), [\"representation\"]], bins=200, x=\"representation\", color=\"orange\", label=\"naml\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"npa\"), [\"representation\"]], bins=200, x=\"representation\", color=\"purple\", label=\"npa\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"pop\"), [\"representation\"]], bins=200, x=\"representation\", color=\"skyblue\", label=\"pop\", kde=False)\n","\n","plt.title(\"Representation distribution\")\n","  \n","plt.legend(['lstur', 'nrms', 'naml', 'npa', 'pop'])\n","  \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b424610f-b497-4b91-9587-f8c7932bb7a0","showTitle":false,"title":""}},"outputs":[],"source":["df_representation_all.groupby('algo').agg(F.mean('representation').alias('mean_representation'), \n","                                          F.stddev('representation').alias('stddev_representation')).show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = df_fragmentation_all.toPandas()\n","  \n","sns.histplot(data=df.loc[(df[\"algo\"] == \"lstur\"), [\"fragmentation\"]], bins=200, x=\"fragmentation\", color=\"red\", label=\"lstur\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"nrms\"), [\"fragmentation\"]], bins=200, x=\"fragmentation\", color=\"green\", label=\"nrms\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"naml\"), [\"fragmentation\"]], bins=200, x=\"fragmentation\", color=\"orange\", label=\"naml\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"npa\"), [\"fragmentation\"]], bins=200, x=\"fragmentation\", color=\"purple\", label=\"npa\", kde=False)\n","sns.histplot(data=df.loc[(df[\"algo\"] == \"pop\"), [\"fragmentation\"]], bins=200, x=\"fragmentation\", color=\"skyblue\", label=\"pop\", kde=False)\n","\n","plt.title(\"Fragmentation distribution\")\n","  \n","plt.legend(['lstur', 'nrms', 'naml', 'npa', 'pop'])\n","  \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f0bd701c-c4af-4a11-a3d7-f5f7e30bd0a1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>algo</th><th>mean_fragmentation</th><th>stddev_fragmentation</th></tr></thead><tbody><tr><td>lstur</td><td>0.8051367747077504</td><td>0.03334955976549406</td></tr><tr><td>pop</td><td>0.7808469677567781</td><td>0.031611804226194036</td></tr><tr><td>naml</td><td>0.7886399536233348</td><td>0.036384301733478334</td></tr><tr><td>npa</td><td>0.7969852605739273</td><td>0.033223707022773774</td></tr><tr><td>nrms</td><td>0.7924785881139604</td><td>0.034227846507726103</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["lstur",0.8051367747077504,0.03334955976549406],["pop",0.7808469677567781,0.031611804226194036],["naml",0.7886399536233348,0.036384301733478334],["npa",0.7969852605739273,0.033223707022773774],["nrms",0.7924785881139604,0.034227846507726103]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"algo","type":"\"string\""},{"metadata":"{}","name":"mean_fragmentation","type":"\"double\""},{"metadata":"{}","name":"stddev_fragmentation","type":"\"double\""}],"type":"table"}},"output_type":"display_data"}],"source":["display(df_fragmentation_all.groupby('algo').agg(F.mean('fragmentation').alias('mean_fragmentation'), F.stddev('fragmentation').alias('stddev_fragmentation')))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6721f762-6dea-4c47-98ed-04b0e93897b8","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"RADio metrec with MIND","notebookOrigID":3720487088895143,"widgets":{}},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"vscode":{"interpreter":{"hash":"38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"}}},"nbformat":4,"nbformat_minor":0}
